# national boundaries
map('usa')
# county map of Texas
map('county', 'Texas')
# map of three states
map('state', region = c('Texas', 'New Mexico', 'Arizona'))
# show the effect of myborder = 0
map.axes()
# Bonne equal-area projection of states
map('state', proj = 'bonne', param = 45)
# map of the dakotas
map("state", ".*dakota", myborder = 0)
# names of the San Juan islands in Washington state
map('county', 'washington,san', names = TRUE, plot = FALSE)
# national boundaries in one linetype, states in another
map("state", interior = FALSE)
map("state", boundary = FALSE, lty = 2, add = TRUE)
# plot the ozone data on a base map
data(ozone)
View(ozone)
map("state", xlim = range(ozone$x), ylim = range(ozone$y))
# put in x,y position the median value
text(ozone$x, ozone$y, ozone$median)
box()
# mapproj is used here for projection="polyconic"
# color US county map by 2009 unemployment rate
# match counties to map using FIPS county codes
# load unemp data which includes data for some counties not on the "lower 48 states" county map,
# such as those in Alaska, Hawaii, Puerto Rico, and some tiny Virginia cities
data(unemp)
data(county.fips)
# define color buckets
colors = c("#F1EEF6", "#D4B9DA", "#C994C7", "#DF65B0", "#DD1C77", "#980043")
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8, 10, 100)))
leg.txt <- c("<2%", "2-4%", "4-6%", "6-8%", "8-10%", ">10%")
# align data with map definitions by (partial) matching state,county
# names, which include multiple polygons for some counties
cnty.fips <- county.fips$fips[match(map("county", plot=FALSE)$names,
county.fips$polyname)]
colorsmatched <- unemp$colorBuckets [match(cnty.fips, unemp$fips)]
# draw map
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0,
lwd = 0.25, projection = "polyconic")
map("state", col = "black", fill = FALSE, add = TRUE, lwd = 1,
projection="polyconic")
title("unemployment by county, 2009")
legend("bottomleft", leg.txt, cex=0.60, horiz = FALSE, fill = colors)
#---------------------------------#
#- Data Analysis & Graphics in R -#
#-------Statistical Analyses------#
#---------------------------------#
#Linear Correlation#
# Correlation is used to test for a relationship
# between two numerical variables or two ranked (ordinal) variables.
# We will begin with a data set called "cats" from the "MASS" library,
# which contains information on various features of house cats.
# Please install package if not already installed
# install.packages('MASS')
library("MASS")
data(cats)
View(cats)
summary(cats)
with(cats, plot(Bwt, Hwt))
title(main="Heart Weight (g) vs. Body Weight (kg)\nof Domestic Cats")
# Pearson's Correlation (r):
# The value of r is such that -1 < r < +1.
# The + and – signs are used for +ve & -ve correlations respectively.
linear correlations and negative linear correlations, respectively
with(cats, cor(Bwt, Hwt))
#coefficient of determination:
# The coefficient of determination is such that 0<(r)^2<1,
# and denotes the strength of the linear association between x and y.
with(cats, cor(Bwt, Hwt))^2
# test of significance:
# to decide whether the linear relationship is strong enough
# If the test concludes that the correlation coefficient is significantly different from 0,
# we say that the correlation coefficient is "significant".
with(cats, cor.test(Bwt, Hwt))
#For positive correlation:
with(cats, cor.test(Bwt, Hwt, alternative="greater", conf.level=.8))
#For a more revealing scatterplot:
with(cats, plot(Bwt, Hwt,
xlab="Body Weight in kg",
ylab="Heart Weight in g",
main="Heart Weight vs. Body Weight of Cats"))
with(cats,points(Bwt[Sex=="F"],Hwt[Sex=="F"],pch=16,col="red"))
with(cats,points(Bwt[Sex=="M"],Hwt[Sex=="M"],pch=17,col="blue"))
legend("topleft", c('Females','Males'),
cex=0.60, horiz = FALSE, pch=c(16,17),
col=c('red','blue'))
# Correlation and Covariance Matrices#
# If a data frame (or other table-like object) contains
# more than two numerical variables, then the cor( ) function
# will result in a correlation matrix.
rm(cats) # if you haven't already
data(cement) # also in the MASS library
View(cement)
str(cement)
# correlation matrix
cor(cement)
# If you prefer a covariance matrix, use cov( )...
cov(cement)
# If you have a covariance matrix and want a correlation matrix...
cov.matr = cov(cement)
cov2cor(cov.matr)
# If you want a visual representation of the correlation matrix
# (i.e., a scatterplot matrix)...
pairs(cement)
# The command plot(cement) would also have done the same thing.
plot(cement)
#Correlations for Ranked Data#
ls()
rm(cement, cov.matr)                 # clean up first
coach1 = c(1,2,3,4,5,6,7,8,9,10)
coach2 = c(4,8,1,5,9,2,10,7,3,6)
ls()
# Spearman's Rank Correlation
cor(coach1, coach2, method="spearman")
#test of significance by spearman
cor.test(coach1, coach2, method="spearman")
# Kendall's Rank Correlation
cor(coach1, coach2, method="kendall")
cor.test(coach1, coach2, method="kendall")
ls()
rm(coach1,coach2)                    # clean up again
#Linear Regression#
# Std. Error is the standard deviation of the sampling distribution.
# 't value' is the value of the t-statistic for
# testing whether the corresponding regression
# coefficient is different from 0.
# Pr. is the p-value for the hypothesis test for
# which the t value is the test statistic.
# If that probability is low, it's suggesting that
# it would be rare to get a result.
data(cats)
# as we removed cats, attach back cats
attach(cats)
# Liner regression
(lm.out = lm(Hwt ~ Bwt))
summary(lm.out)
# turn off the significance stars
options(show.signif.stars=F)
# shows an ANOVA table
# here Pr is insignificant (i.e. Hwt and Bwt association is low)
anova(lm.out)
# plotting the regression line on a scatterplot:
plot(Hwt ~ Bwt, main="Cat Hwt-Bwt Regression Plot")
abline(lm.out, col="red")
# make a copy of current settings
mypar <- par()
par(mfrow=c(2,2))
plot(lm.out)
cats[144,]
lm.out$fitted[144]
lm.out$residuals[144]
# if you haven't already done this
par(mfrow=c(1,1))
# Cook's Distance plot converts this to a standardized residual
plot(cooks.distance(lm.out), main="Cat Hwt-Bwt Cook's Distance Plot")
# Case 144 tops the charts. Look at the regression coefficients
# without the outlying point in the model.
lm.without144 = lm(Hwt ~ Bwt, subset=(Hwt<20.5))
lm.without144
# Another is to use a procedure that is robust
# in the face of outlying points.
rlm(Hwt ~ Bwt)
# restore original settings
par(mypar)
#ANOVA#
# Looking at the breakdown of variance in the outcome variable.
# Quick, easy way to rule out un-needed variables
# that contribute little to the explanation of a dependent variable
# For a significant model, we need a P value of
# equal or smaller than 0.05 ( for 95% ).
# Import cars dataset:
cars <- read.csv('./data/Cars.csv')
head(cars)
# Oneway anova
a1 <- aov(HighwayMPG~Origin,data=cars)
summary(a1)
# Twoway anova
a2 <- aov(HighwayMPG~Origin+Type,data=cars)
summary(a2) # display Type I ANOVA table
drop1(a2,~.,test='F') # display type III SS and F Tests
# Twoway anova (with interaction between independent variables)
a3 <- aov(HighwayMPG~Origin*Type,data=cars)
summary(a3) # display Type I ANOVA table
drop1(a3,~.,test='F') # display type III SS and F Tests
# Plotting categorical-by-categorical interactions:
interaction.plot(cars$Origin,cars$Type,cars$HighwayMPG)
interaction.plot(cars$Origin,cars$Type,cars$HighwayMPG,pch=c(1,15,4,16,0,18),type='b')
# ANOVA to compare nested models
mymodel1 <- lm(HighwayMPG~Origin+Weight,data=cars)
mymodel2 <- lm(HighwayMPG~Origin+Weight+EngineSize+Price,data=cars)
anova(mymodel1,mymodel2)
#Logistic regression#
# It is a statistical method for analyzing a dataset
# in which there are one or more independent variables
# that determine an outcome. The outcome is measured with
# a dichotomous variable (in which there are only two possible outcomes)
adm <- read.csv('./data/student_admission.csv')
head(adm)
View(adm)
adm$rank <- factor(adm$rank)
logit1 <- glm(admit~rank,data=adm,family='binomial')
summary(logit1)
logit2 <- glm(admit~gpa,data=adm,family='binomial')
summary(logit2)
logit3 <- glm(admit~rank+gre+gpa,data=adm,family='binomial')
summary(logit3)
# Plotting predicted probabilities for continuous predictor
mypp <- data.frame(gpa=sort(adm$gpa))
plot(adm$gpa,adm$admit,pch=16,cex=.5)
lines(mypp$gpa,predict(logit2,newdata=mypp,type='response'))
#---------------------------------#
#- Data Analysis & Graphics in R -#
#-------Statistical Analyses------#
#---------------------------------#
#Linear Correlation#
# Correlation is used to test for a relationship
# between two numerical variables or two ranked (ordinal) variables.
# We will begin with a data set called "cats" from the "MASS" library,
# which contains information on various features of house cats.
# Please install package if not already installed
# install.packages('MASS')
library("MASS")
data(cats)
View(cats)
summary(cats)
with(cats, plot(Bwt, Hwt))
title(main="Heart Weight (g) vs. Body Weight (kg)\nof Domestic Cats")
# Pearson's Correlation (r):
# The value of r is such that -1 < r < +1.
# The + and – signs are used for +ve & -ve correlations respectively.
# linear correlations and negative linear correlations, respectively
with(cats, cor(Bwt, Hwt))
#coefficient of determination:
# The coefficient of determination is such that 0<(r)^2<1,
# and denotes the strength of the linear association between x and y.
with(cats, cor(Bwt, Hwt))^2
# test of significance:
# to decide whether the linear relationship is strong enough
# If the test concludes that the correlation coefficient is significantly different from 0,
# we say that the correlation coefficient is "significant".
with(cats, cor.test(Bwt, Hwt))
#For positive correlation:
with(cats, cor.test(Bwt, Hwt, alternative="greater", conf.level=.8))
#For a more revealing scatterplot:
with(cats, plot(Bwt, Hwt,
xlab="Body Weight in kg",
ylab="Heart Weight in g",
main="Heart Weight vs. Body Weight of Cats"))
with(cats,points(Bwt[Sex=="F"],Hwt[Sex=="F"],pch=16,col="red"))
with(cats,points(Bwt[Sex=="M"],Hwt[Sex=="M"],pch=17,col="blue"))
legend("topleft", c('Females','Males'),
cex=0.60, horiz = FALSE, pch=c(16,17),
col=c('red','blue'))
# Correlation and Covariance Matrices#
# If a data frame (or other table-like object) contains
# more than two numerical variables, then the cor( ) function
# will result in a correlation matrix.
rm(cats) # if you haven't already
data(cement) # also in the MASS library
View(cement)
str(cement)
# correlation matrix
cor(cement)
# If you prefer a covariance matrix, use cov( )...
cov(cement)
# If you have a covariance matrix and want a correlation matrix...
cov.matr = cov(cement)
cov2cor(cov.matr)
# If you want a visual representation of the correlation matrix
# (i.e., a scatterplot matrix)...
pairs(cement)
# The command plot(cement) would also have done the same thing.
plot(cement)
#Correlations for Ranked Data#
ls()
rm(cement, cov.matr)                 # clean up first
coach1 = c(1,2,3,4,5,6,7,8,9,10)
coach2 = c(4,8,1,5,9,2,10,7,3,6)
ls()
# Spearman's Rank Correlation
cor(coach1, coach2, method="spearman")
#test of significance by spearman
cor.test(coach1, coach2, method="spearman")
# Kendall's Rank Correlation
cor(coach1, coach2, method="kendall")
cor.test(coach1, coach2, method="kendall")
ls()
rm(coach1,coach2)                    # clean up again
#Linear Regression#
# Std. Error is the standard deviation of the sampling distribution.
# 't value' is the value of the t-statistic for
# testing whether the corresponding regression
# coefficient is different from 0.
# Pr. is the p-value for the hypothesis test for
# which the t value is the test statistic.
# If that probability is low, it's suggesting that
# it would be rare to get a result.
data(cats)
# as we removed cats, attach back cats
attach(cats)
# Liner regression
(lm.out = lm(Hwt ~ Bwt))
summary(lm.out)
# turn off the significance stars
options(show.signif.stars=F)
# shows an ANOVA table
# here Pr is insignificant (i.e. Hwt and Bwt association is low)
anova(lm.out)
# plotting the regression line on a scatterplot:
plot(Hwt ~ Bwt, main="Cat Hwt-Bwt Regression Plot")
abline(lm.out, col="red")
# make a copy of current settings
mypar <- par()
par(mfrow=c(2,2))
plot(lm.out)
cats[144,]
lm.out$fitted[144]
lm.out$residuals[144]
# if you haven't already done this
par(mfrow=c(1,1))
# Cook's Distance plot converts this to a standardized residual
plot(cooks.distance(lm.out), main="Cat Hwt-Bwt Cook's Distance Plot")
# Case 144 tops the charts. Look at the regression coefficients
# without the outlying point in the model.
lm.without144 = lm(Hwt ~ Bwt, subset=(Hwt<20.5))
lm.without144
# Another is to use a procedure that is robust
# in the face of outlying points.
rlm(Hwt ~ Bwt)
# restore original settings
par(mypar)
#ANOVA#
# Looking at the breakdown of variance in the outcome variable.
# Quick, easy way to rule out un-needed variables
# that contribute little to the explanation of a dependent variable
# For a significant model, we need a P value of
# equal or smaller than 0.05 ( for 95% ).
# Import cars dataset:
cars <- read.csv('./data/Cars.csv')
head(cars)
# Oneway anova
a1 <- aov(HighwayMPG~Origin,data=cars)
summary(a1)
# Twoway anova
a2 <- aov(HighwayMPG~Origin+Type,data=cars)
summary(a2) # display Type I ANOVA table
drop1(a2,~.,test='F') # display type III SS and F Tests
# Twoway anova (with interaction between independent variables)
a3 <- aov(HighwayMPG~Origin*Type,data=cars)
summary(a3) # display Type I ANOVA table
drop1(a3,~.,test='F') # display type III SS and F Tests
# Plotting categorical-by-categorical interactions:
interaction.plot(cars$Origin,cars$Type,cars$HighwayMPG)
interaction.plot(cars$Origin,cars$Type,cars$HighwayMPG,pch=c(1,15,4,16,0,18),type='b')
# ANOVA to compare nested models
mymodel1 <- lm(HighwayMPG~Origin+Weight,data=cars)
mymodel2 <- lm(HighwayMPG~Origin+Weight+EngineSize+Price,data=cars)
anova(mymodel1,mymodel2)
#Logistic regression#
# It is a statistical method for analyzing a dataset
# in which there are one or more independent variables
# that determine an outcome. The outcome is measured with
# a dichotomous variable (in which there are only two possible outcomes)
adm <- read.csv('./data/student_admission.csv')
head(adm)
View(adm)
adm$rank <- factor(adm$rank)
logit1 <- glm(admit~rank,data=adm,family='binomial')
summary(logit1)
logit2 <- glm(admit~gpa,data=adm,family='binomial')
summary(logit2)
logit3 <- glm(admit~rank+gre+gpa,data=adm,family='binomial')
summary(logit3)
# Plotting predicted probabilities for continuous predictor
mypp <- data.frame(gpa=sort(adm$gpa))
plot(adm$gpa,adm$admit,pch=16,cex=.5)
lines(mypp$gpa,predict(logit2,newdata=mypp,type='response'))
#---------------------------------------------#
#--------Data Analysis & Graphics in R--------#
#----------------Basic ML in R----------------#
#---K Means, KNN, PCA, LDA, SVM, NaiveBayes---#
#---------------------------------------------#
# First we need to load up some packages to support ML.
# Please install package if not already installed
#install.packages('MASS')
#install.packages('ggplot2')
#install.packages('scales')
#install.packages('gridExtra')
library(MASS)
library(ggplot2)
library(scales)
library(gridExtra)
library(class)
library(e1071)
#load data
data(iris)
View(iris)
summary(iris)
# Prepare training and testing data
testidx <- which(1:length(iris[,1])%%5 == 0)
iristrain <- iris[-testidx,]
iristest <- iris[testidx,]
# OK. Lets get to it
# K-Means
# kmeans(), built into the R base package, is an unsupervised
# learning technique. The goal is to cluster the observed data
# into groups. This is achieved by assuming a Euclidean distance
# metric, and finding points which lie at local centroids. All
# points are then assigned to their closest centroid and are
# thus clustered. The algorithmic approach to finding these
# centroids is to pick k points at random then assign all other
# points to the centroids. The algorithm then chooses new
# centroids based on the mean point of the resulting clusters.
# Then with these new centroids, the remaining N-k points are
# reclustered. This repeats until some stopping condition is
# reached.
# Here we know, a priori, that there are 3 clusters, so we set
# k = 3.
newiris <- iris
newiris$Species <- NULL
(kc <- kmeans(newiris, 3))
table(iris$Species, kc$cluster)
plot(newiris[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
points(kc$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=2)
# K Nearest Neighbor
# In KNN we classify an unknown point by looking at k nearest
# neighbors of known classification. In the case of k=1, we
# find the closest point in our training set to a new point
# from our test set. We then assume that the new point has
# the same class as its closest neighbor in the test set.
# For k>1, we typically apply a voting mechanism to pick the
# modal class from the neighborhood of points in the training
# set.
train_input <- as.matrix(iristrain[,-5])
train_output <- as.vector(iristrain[,5])
test_input <- as.matrix(iristest[,-5])
prediction <- knn(train_input, test_input,
train_output, k=5)
table(prediction, iristest$Species)
# PCA - We use principle component analysis (PCA) to transform our space.
# PCA transforms the space by looking at the vectors along which the
# bulk of the variance in the data occur. The vector that embodies
# the greatest variance becomes the first principle component axis
# in the transformed space. The second axis then is formed along the
# vector that is orthogonal to the first but with the second most
# variance in the data. And so on.
# LDA - In linear discriminant analysis (LDA) we no longer look for recursive
# partitions, but rather for lines that go between the clusters.
# In some ways, this is similar to KNN. LDA approaches
# the problem by applying a transform that applies the inverse of
# the estimated covariance matrix. In this transformed space,
# classification is finding the closest cluster mean.
# Load data for PCA
pca <- prcomp(iris[,-5],
center = TRUE,
scale. = TRUE)
# Scaled PCA
prop.pca = pca$sdev^2/sum(pca$sdev^2)
# Load data for LDAA
lda <- lda(Species ~ .,
iris,
prior = c(1,1,1)/3)
# Scaled LDA
prop.lda = lda$svd^2/sum(lda$svd^2)
# Predict LDA
plda <- predict(object = lda,
newdata = iris)
# generate dataset to compare PCA and LDA
dataset = data.frame(species = iris[,"Species"],
pca = pca$x, lda = plda$x)
# generate plot for LDA
p1 <- ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = species, shape = species), size = 2.5) +
labs(x = paste("LD1 (", percent(prop.lda[1]), ")", sep=""),
y = paste("LD2 (", percent(prop.lda[2]), ")", sep=""))
# generate plot for PCA
p2 <- ggplot(dataset) + geom_point(aes(pca.PC1, pca.PC2, colour = species, shape = species), size = 2.5) +
labs(x = paste("PC1 (", percent(prop.pca[1]), ")", sep=""),
y = paste("PC2 (", percent(prop.pca[2]), ")", sep=""))
grid.arrange(p1, p2)
# NaiveBayes#
# Computes the conditional a-posterior probabilities
# of a categorical class variable given independent
# predictor variables using the Bayes rule.
# Load data
data(iris)
(m <- naiveBayes(iris[,-5], iris[,5]))
table(predict(m, iris[,-5]), iris[,5])
# SVM
# Support vector machines take the next step from LDA/QDA. However
# instead of making linear voronoi boundaries between the cluster
# means, we concern ourselves primarily with the points on the
# boundaries between the clusters. These boundary points define
# the 'support vector'. Between two completely separable clusters
# there are two support vectors and a margin of empty space
# between them. The SVM optimization technique seeks to maximize
# the margin by choosing a hyperplane between the support vectors
# of the opposing clusters. For non-separable clusters, a slack
# constraint is added to allow for a small number of points to
# lie inside the margin space.
# Load data
data(cats)
m1 <- svm(Sex~., data = cats)
plot(m1, cats)
cplus<-cats
m2 <- svm(Sex~., data = cplus)
plot(m2, cplus, Bwt~Hwt)
library(ggplot2); library(scales); library(grid); library(RColorBrewer)
library(spatstat)
